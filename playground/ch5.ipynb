{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.9.0\n",
      "numpy version: 1.26.3\n",
      "tiktoken version: 0.7.0\n",
      "torch version: 2.3.1+cu118\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "pkgs = [\"matplotlib\", \n",
    "        \"numpy\", \n",
    "        \"tiktoken\", \n",
    "        \"torch\",\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model from previous chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed-forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
    "print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve target probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "source": [
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative log probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Logits have shape (batch_size, num_tokens, vocab_size)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "# Targets have shape (batch_size, num_tokens)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(48725.8203)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Calculating the training and validation set losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i+1:i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                      stride=128, shuffle=True, drop_last=True,\n",
    "                      num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle,\n",
    "                            drop_last=drop_last,\n",
    "                            num_workers=num_workers)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An optional check that the data was loaded correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another optional check that the token sizes are in the expected ballpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 4608\n",
      "Validation tokens: 512\n",
      "All tokens: 5120\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.987583584255642\n",
      "Validation loss: 10.981104850769043\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "# train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "#     model, train_loader, val_loader, optimizer, device,\n",
    "#     num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "#     start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "# def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "#     fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "#     # Plot training and validation loss against epochs\n",
    "#     ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "#     ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "#     ax1.set_xlabel(\"Epochs\")\n",
    "#     ax1.set_ylabel(\"Loss\")\n",
    "#     ax1.legend(loc=\"upper right\")\n",
    "#     ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "#     # Create a second x-axis for tokens seen\n",
    "#     ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "#     ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "#     ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "#     fig.tight_layout()  # Adjust layout to make room\n",
    "#     plt.savefig(\"loss-plot.pdf\")\n",
    "#     plt.show()\n",
    "\n",
    "# epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "# plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results above, we can see that the model starts out generating incomprehensible strings of words, whereas towards the end, it's able to produce grammatically more or less correct sentences\n",
    "\n",
    "However, based on the training and validation set losses, we can see that the model starts overfitting\n",
    "\n",
    "If we were to check a few passages it writes towards the end, we would find that they are contained in the training set verbatim -- it simply memorizes the training data\n",
    "\n",
    "Later, we will cover decoding strategies that can mitigate this memorization by a certain degree\n",
    "Note that the overfitting here occurs because we have a very, very small training set, and we iterate over it so many times\n",
    "\n",
    "The LLM training here primarily serves educational purposes; we mainly want to see that the model can learn to produce coherent text\n",
    "\n",
    "Instead of spending weeks or months on training this model on vast amounts of expensive hardware, we load pretrained weights later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding strategies to control randomness\n",
    "2 techniques: temperature scaling and top-k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren Mortgage TT remember gard ACTIONSussedOND Land Engeleddedemate breaths proxies GalaxyForm\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "# Suppose input is \"every effort moves you\", and the LLM\n",
    "# returns the following logits for the next token:\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "\n",
    "# The next generated token is then as follows:\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toward\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.0907e-02, 1.6313e-03, 1.0019e-04, 5.7212e-01, 3.4190e-03, 1.3257e-04,\n",
      "        1.0120e-04, 3.5758e-01, 4.0122e-03])\n"
     ]
    }
   ],
   "source": [
    "print(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 x closer\n",
      "2 x every\n",
      "0 x effort\n",
      "544 x forward\n",
      "2 x inches\n",
      "1 x moves\n",
      "0 x pizza\n",
      "376 x toward\n",
      "4 x you\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123) # Manual seed for reproducibility\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of determining the most likely token via torch.argmax, we use torch.multinomial(probas, num_samples=1) to determine the most likely token by sampling from the softmax distribution\n",
    "\n",
    "For illustration purposes, let's see what happens when we sample the next token 1,000 times using the original softmax probabilities:\n",
    "\n",
    "We can control the distribution and selection process via a concept called temperature scaling\n",
    "\"Temperature scaling\" is just a fancy word for dividing the logits by a number greater than 0\n",
    "\n",
    "Temperatures greater than 1 will result in more uniformly distributed token probabilities after applying the softmax\n",
    "\n",
    "Temperatures smaller than 1 will result in more confident (sharper or more peaky) distributions after applying the softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "# Temperature values\n",
    "temperatures = [1, 0.1, 5, 50]  # Original, higher confidence, and lower confidence\n",
    "\n",
    "# Calculate scaled probabilities\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f'Temperature = {T}')\n",
    "\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"temperature-plot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the rescaling via temperature 0.1 results in a sharper distribution, approaching torch.argmax, such that the most likely word is almost always selected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "992 x forward\n",
      "0 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "8 x toward\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(scaled_probas[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rescaled probabilities via temperature 5 are more uniformly distributed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153 x closer\n",
      "68 x every\n",
      "55 x effort\n",
      "223 x forward\n",
      "102 x inches\n",
      "50 x moves\n",
      "43 x pizza\n",
      "218 x toward\n",
      "88 x you\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(scaled_probas[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming an LLM input \"every effort moves you\", using the approach above can sometimes result in nonsensical texts, such as \"every effort moves you pizza\", 3.2% of the time (32 out of 1000 times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133 x closer\n",
      "98 x every\n",
      "117 x effort\n",
      "128 x forward\n",
      "114 x inches\n",
      "96 x moves\n",
      "97 x pizza\n",
      "126 x toward\n",
      "91 x you\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(scaled_probas[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input=torch.tensor(float('-inf')), \n",
    "    other=next_token_logits\n",
    ")\n",
    "\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modifying the text generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you inferred rolleduint fabricationagos remarkably hereuced saints freewaylookOkayRand salary baseless\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and saving model weights in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }, \n",
    "    \"model_and_optimizer.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\", weights_only=True)\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading pretrained weights from OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.26.3\n",
      "torch version: 2.3.1+cu118\n",
      "transformers version: 4.41.2\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"numpy\", \"torch\", \"transformers\"]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 14\u001b[0m\n\u001b[1;32m      5\u001b[0m model_names \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2-small (124M)\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai-community/gpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2-medium (355M)\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai-community/gpt2-medium\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2-large (774M)\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai-community/gpt2-large\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2-xl (1558M)\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai-community/gpt2-xl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m }\n\u001b[1;32m     12\u001b[0m CHOOSE_MODEL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2-large (774M)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 14\u001b[0m gpt_hf \u001b[38;5;241m=\u001b[39m \u001b[43mGPT2Model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mCHOOSE_MODEL\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpoints\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m gpt_hf\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/study/LLM-from-scratch/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3351\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3335\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   3336\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m   3337\u001b[0m     cached_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3338\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_dir,\n\u001b[1;32m   3339\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_download\u001b[39m\u001b[38;5;124m\"\u001b[39m: force_download,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3349\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m: commit_hash,\n\u001b[1;32m   3350\u001b[0m     }\n\u001b[0;32m-> 3351\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcached_file_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3353\u001b[0m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[1;32m   3354\u001b[0m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[1;32m   3355\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[1;32m   3356\u001b[0m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[0;32m~/study/LLM-from-scratch/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:399\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 399\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    414\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m~/study/LLM-from-scratch/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/study/LLM-from-scratch/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1221\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m   1203\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m   1204\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1218\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1219\u001b[0m     )\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/study/LLM-from-scratch/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1367\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, headers, proxies, etag_timeout, endpoint, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1365\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1367\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1377\u001b[0m     _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n",
      "File \u001b[0;32m~/study/LLM-from-scratch/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1884\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[1;32m   1881\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m   1882\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m-> 1884\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1891\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1893\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1894\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[0;32m~/study/LLM-from-scratch/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:539\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    537\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[1;32m    540\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    541\u001b[0m             progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m~/study/LLM-from-scratch/.venv/lib/python3.10/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/study/LLM-from-scratch/.venv/lib/python3.10/site-packages/urllib3/response.py:1043\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1043\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1045\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1046\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/study/LLM-from-scratch/.venv/lib/python3.10/site-packages/urllib3/response.py:935\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 935\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/study/LLM-from-scratch/.venv/lib/python3.10/site-packages/urllib3/response.py:862\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    859\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 862\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/study/LLM-from-scratch/.venv/lib/python3.10/site-packages/urllib3/response.py:845\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m    843\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    844\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1303\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1300\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1301\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1302\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1159\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Model\n",
    "\n",
    "\n",
    "# allowed model names\n",
    "model_names = {\n",
    "    \"gpt2-small (124M)\": \"openai-community/gpt2\",\n",
    "    \"gpt2-medium (355M)\": \"openai-community/gpt2-medium\",\n",
    "    \"gpt2-large (774M)\": \"openai-community/gpt2-large\",\n",
    "    \"gpt2-xl (1558M)\": \"openai-community/gpt2-xl\"\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "\n",
    "gpt_hf = GPT2Model.from_pretrained(model_names[CHOOSE_MODEL], cache_dir=\"checkpoints\")\n",
    "gpt_hf.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"drop_rate\": 0.0,       # Dropout rate\n",
    "    \"qkv_bias\": True        # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_check(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(right.clone().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_weights(gpt, gpt_hf):\n",
    "\n",
    "    d = gpt_hf.state_dict()\n",
    "\n",
    "    gpt.pos_emb.weight = assign_check(gpt.pos_emb.weight, d[\"wpe.weight\"])\n",
    "    gpt.tok_emb.weight = assign_check(gpt.tok_emb.weight, d[\"wte.weight\"])\n",
    "    \n",
    "    for b in range(BASE_CONFIG[\"n_layers\"]):\n",
    "        q_w, k_w, v_w = np.split(d[f\"h.{b}.attn.c_attn.weight\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign_check(gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign_check(gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign_check(gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "    \n",
    "        q_b, k_b, v_b = np.split(d[f\"h.{b}.attn.c_attn.bias\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign_check(gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign_check(gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign_check(gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "    \n",
    "    \n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign_check(gpt.trf_blocks[b].att.out_proj.weight, d[f\"h.{b}.attn.c_proj.weight\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign_check(gpt.trf_blocks[b].att.out_proj.bias, d[f\"h.{b}.attn.c_proj.bias\"])\n",
    "    \n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign_check(gpt.trf_blocks[b].ff.layers[0].weight, d[f\"h.{b}.mlp.c_fc.weight\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign_check(gpt.trf_blocks[b].ff.layers[0].bias, d[f\"h.{b}.mlp.c_fc.bias\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign_check(gpt.trf_blocks[b].ff.layers[2].weight, d[f\"h.{b}.mlp.c_proj.weight\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign_check(gpt.trf_blocks[b].ff.layers[2].bias, d[f\"h.{b}.mlp.c_proj.bias\"])\n",
    "    \n",
    "        gpt.trf_blocks[b].norm1.scale = assign_check(gpt.trf_blocks[b].norm1.scale, d[f\"h.{b}.ln_1.weight\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign_check(gpt.trf_blocks[b].norm1.shift, d[f\"h.{b}.ln_1.bias\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign_check(gpt.trf_blocks[b].norm2.scale, d[f\"h.{b}.ln_2.weight\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign_check(gpt.trf_blocks[b].norm2.shift, d[f\"h.{b}.ln_2.bias\"])\n",
    "    \n",
    "        gpt.final_norm.scale = assign_check(gpt.final_norm.scale, d[f\"ln_f.weight\"])\n",
    "        gpt.final_norm.shift = assign_check(gpt.final_norm.shift, d[f\"ln_f.bias\"])\n",
    "        gpt.out_head.weight = assign_check(gpt.out_head.weight, d[\"wte.weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "gpt = GPTModel(BASE_CONFIG)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "load_weights(gpt, gpt_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves forward, but it's not enough.\n",
      "\n",
      "\"I'm not going to sit here and say, 'I'm not going to do this,'\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt.to(device),\n",
    "    idx=text_to_token_ids(\"Every effort moves\", tokenizer).to(device),\n",
    "    max_new_tokens=30,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    top_k=1,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 5.1\n",
    "Use the print_sampled_tokens function to print the sampling frequencies of the softmax probabilities scaled with the temper atures shown in Figure 5.13. How often is the word \"pizza\" sampled in each case? Can you think of a faster and more accurate way to determine how often the word \"pizza\" is sampled?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the number of times the word \"pizza\" is sampled using the print_sampled_tokens function we defined in this section. Let's start with the code we defined in section 5.3.1.\n",
    "\n",
    "It is sampled 0x if the temperature is 0 or 0.1, and it is sampled 32x if the temperature is scaled up to 5. The estimated probability is 32/1000 * 100% = 3.2%.\n",
    "\n",
    "The actual probability is 4.3% and contained in the rescaled softmax probability tensor (scaled_probas[2][6])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123)\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "\n",
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "\n",
    "temperatures = [1, 0.1, 5]  # Original, higher, and lower temperature\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, probas in enumerate(scaled_probas):\n",
    "    print(\"\\n\\nTemperature:\", temperatures[i])\n",
    "    print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that sampling offers an approximation of the actual probabilities when the word \"pizza\" is sampled. E.g., if it is sampled 32/1000 times, the estimated probability is 3.2%. To obtain the actual probability, we can check the probabilities directly by accessing the corresponding entry in scaled_probas.\n",
    "\n",
    "Since \"pizza\" is the 7th entry in the vocabulary, for the temperature of 5, we obtain it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp5_idx = 2\n",
    "pizza_idx = 6\n",
    "\n",
    "scaled_probas[temp5_idx][pizza_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 5.2\n",
    "Play around with different temperatures and top-k settings. Based on your observations, can you think of applications where lower temperature and top-k settings are desired? Vice versa, can you think of applications where higher temperature and top-k settings are preferred? (It's recommended to also revisit this exercise at the end of the chapter after loading the pretrained weights from OpenAI.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=5,\n",
    "    temperature=5\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both temperature and top-k settings have to be adjusted based on the individual LLM (a kind of trial and error process until it generates desirable outputs)\n",
    "\n",
    "The desirable outcomes are also application-specific, though\n",
    "\n",
    "Lower top-k and temperatures result in less random outcomes, which is desired when creating educational content, technical writing or question answering, data analyses, code generation, and so forth\n",
    "\n",
    "Higher top-k and temperatures result in more diverse and random outputs, which is more desirable for brainstorming tasks, creative writing, and so forth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 5.3\n",
    "What are the different combinations of settings for the generate function to force deterministic behavior, that is, disabling the random sampling such that it always produces the same outputs similar to the generate_simple function?   So far, we covered how to pretrain LLMs and use them to generate text. The last two sections of this chapter will discuss how we save and load the trained LLM and how we load pretrained weights from OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deterministic behavior: No top_k, no temperature scaling\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=None,\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 5.4\n",
    "After saving the weights, load the model and optimizer in a new Python session or Jupyter notebook file and continue pretraining it for 1 more epoch using the train_model_simple function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "checkpoint = torch.load(\"model_and_optimizer.pth\", weights_only=True)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 5.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
