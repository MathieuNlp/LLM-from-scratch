{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.9.0\n",
      "numpy version: 1.26.3\n",
      "tiktoken version: 0.7.0\n",
      "torch version: 2.3.1+cu118\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "pkgs = [\"matplotlib\", \n",
    "        \"numpy\", \n",
    "        \"tiktoken\", \n",
    "        \"torch\",\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model from previous chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out,\n",
    "                 context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension.\n",
    "        # Then we unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # from (b, num_tokens, num_heads, head_dim) to (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Tensor shape: (b, num_tokens, n_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 *torch.pow(x, 3))))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(d_in=cfg[\"emb_dim\"], \n",
    "                                      d_out=cfg[\"emb_dim\"],\n",
    "                                      context_length=cfg[\"context_length\"],\n",
    "                                      dropout=cfg[\"drop_rate\"],\n",
    "                                      num_heads=cfg[\"n_heads\"],\n",
    "                                      qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"]) # For the Attention module\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"]) # For the FFN module\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = shortcut + x\n",
    "        return x\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range (cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = nn.LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
    "print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve target probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "source": [
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative log probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Logits have shape (batch_size, num_tokens, vocab_size)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "# Targets have shape (batch_size, num_tokens)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(48725.8203)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Calculating the training and validation set losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i+1:i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                      stride=128, shuffle=True, drop_last=True,\n",
    "                      num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle,\n",
    "                            drop_last=drop_last,\n",
    "                            num_workers=num_workers)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An optional check that the data was loaded correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another optional check that the token sizes are in the expected ballpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 4608\n",
      "Validation tokens: 512\n",
      "All tokens: 5120\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.987583584255642\n",
      "Validation loss: 10.981104850769043\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.821, Val loss 9.931\n",
      "Ep 1 (Step 000005): Train loss 8.069, Val loss 8.334\n",
      "Every effort moves you,,,,,,,,,,,,,,.                                   \n",
      "Ep 2 (Step 000010): Train loss 6.623, Val loss 7.049\n",
      "Ep 2 (Step 000015): Train loss 6.046, Val loss 6.598\n",
      "Every effort moves you, and,, and,,,,,,,,,.                                   \n",
      "Ep 3 (Step 000020): Train loss 5.558, Val loss 6.503\n",
      "Ep 3 (Step 000025): Train loss 5.471, Val loss 6.392\n",
      "Every effort moves you, and to the to the of the to the, and I had. Gis, and I had, and, and, and, and I had, and, and, and, and, and, and, and, and, and,\n",
      "Ep 4 (Step 000030): Train loss 4.995, Val loss 6.275\n",
      "Ep 4 (Step 000035): Train loss 4.756, Val loss 6.289\n",
      "Every effort moves you, and I had been the picture.                    \"I\"I the the donkey of the donkey the donkey of the picture and I had been a\"I\n",
      "Ep 5 (Step 000040): Train loss 4.113, Val loss 6.182\n",
      "Every effort moves you know the \"Oh, and he had to me--I me. \"Oh, I felt--and it's the  \"Oh, and I had been the donkey--and it to me, and down the \"Oh,\n",
      "Ep 6 (Step 000045): Train loss 3.721, Val loss 6.143\n",
      "Ep 6 (Step 000050): Train loss 3.170, Val loss 6.147\n",
      "Every effort moves you know the fact, and I felt.  \"I had the last word.     \"I didn't. \"I was his pictures--I looked.   \"I looked.    \"I\n",
      "Ep 7 (Step 000055): Train loss 3.109, Val loss 6.186\n",
      "Ep 7 (Step 000060): Train loss 2.370, Val loss 6.126\n",
      "Every effort moves you know the inevitable garlanded to have to have the fact with a little: \"Yes--and by me to me to have to see a smile behind his pictures--as I had been the honour of the donkey.      \n",
      "Ep 8 (Step 000065): Train loss 1.911, Val loss 6.151\n",
      "Ep 8 (Step 000070): Train loss 1.592, Val loss 6.223\n",
      "Every effort moves you?\"  \"Yes--I glanced after him, and uncertain. \"Oh, he was's an awful simpleton, and Mrs. Gisburn's head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 9 (Step 000075): Train loss 1.239, Val loss 6.259\n",
      "Ep 9 (Step 000080): Train loss 0.957, Val loss 6.270\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. The last word.        He laughed again, and threw back the head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 10 (Step 000085): Train loss 0.708, Val loss 6.387\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results above, we can see that the model starts out generating incomprehensible strings of words, whereas towards the end, it's able to produce grammatically more or less correct sentences\n",
    "\n",
    "However, based on the training and validation set losses, we can see that the model starts overfitting\n",
    "\n",
    "If we were to check a few passages it writes towards the end, we would find that they are contained in the training set verbatim -- it simply memorizes the training data\n",
    "\n",
    "Later, we will cover decoding strategies that can mitigate this memorization by a certain degree\n",
    "Note that the overfitting here occurs because we have a very, very small training set, and we iterate over it so many times\n",
    "\n",
    "The LLM training here primarily serves educational purposes; we mainly want to see that the model can learn to produce coherent text\n",
    "\n",
    "Instead of spending weeks or months on training this model on vast amounts of expensive hardware, we load pretrained weights later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding strategies to control randomness\n",
    "2 techniques: temperature scaling and top-k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "# Suppose input is \"every effort moves you\", and the LLM\n",
    "# returns the following logits for the next token:\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "\n",
    "# The next generated token is then as follows:\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.0907e-02, 1.6313e-03, 1.0019e-04, 5.7212e-01, 3.4190e-03, 1.3257e-04,\n",
      "        1.0120e-04, 3.5758e-01, 4.0122e-03])\n"
     ]
    }
   ],
   "source": [
    "print(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 x closer\n",
      "2 x every\n",
      "0 x effort\n",
      "544 x forward\n",
      "2 x inches\n",
      "1 x moves\n",
      "0 x pizza\n",
      "376 x toward\n",
      "4 x you\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123) # Manual seed for reproducibility\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of determining the most likely token via torch.argmax, we use torch.multinomial(probas, num_samples=1) to determine the most likely token by sampling from the softmax distribution\n",
    "\n",
    "For illustration purposes, let's see what happens when we sample the next token 1,000 times using the original softmax probabilities:\n",
    "\n",
    "We can control the distribution and selection process via a concept called temperature scaling\n",
    "\"Temperature scaling\" is just a fancy word for dividing the logits by a number greater than 0\n",
    "\n",
    "Temperatures greater than 1 will result in more uniformly distributed token probabilities after applying the softmax\n",
    "\n",
    "Temperatures smaller than 1 will result in more confident (sharper or more peaky) distributions after applying the softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "# Temperature values\n",
    "temperatures = [1, 0.1, 5, 50]  # Original, higher confidence, and lower confidence\n",
    "\n",
    "# Calculate scaled probabilities\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f'Temperature = {T}')\n",
    "\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"temperature-plot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the rescaling via temperature 0.1 results in a sharper distribution, approaching torch.argmax, such that the most likely word is almost always selected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "992 x forward\n",
      "0 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "8 x toward\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(scaled_probas[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rescaled probabilities via temperature 5 are more uniformly distributed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153 x closer\n",
      "68 x every\n",
      "55 x effort\n",
      "223 x forward\n",
      "102 x inches\n",
      "50 x moves\n",
      "43 x pizza\n",
      "218 x toward\n",
      "88 x you\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(scaled_probas[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming an LLM input \"every effort moves you\", using the approach above can sometimes result in nonsensical texts, such as \"every effort moves you pizza\", 3.2% of the time (32 out of 1000 times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133 x closer\n",
      "98 x every\n",
      "117 x effort\n",
      "128 x forward\n",
      "114 x inches\n",
      "96 x moves\n",
      "97 x pizza\n",
      "126 x toward\n",
      "91 x you\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(scaled_probas[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input=torch.tensor(float('-inf')), \n",
    "    other=next_token_logits\n",
    ")\n",
    "\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modifying the text generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you know began to my surpriselyed for nothing--I told Mrs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and saving model weights in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 5.1\n",
    "Use the print_sampled_tokens function to print the sampling frequencies of the softmax probabilities scaled with the temper atures shown in Figure 5.13. How often is the word \"pizza\" sampled in each case? Can you think of a faster and more accurate way to determine how often the word \"pizza\" is sampled?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the number of times the word \"pizza\" is sampled using the print_sampled_tokens function we defined in this section. Let's start with the code we defined in section 5.3.1.\n",
    "\n",
    "It is sampled 0x if the temperature is 0 or 0.1, and it is sampled 32x if the temperature is scaled up to 5. The estimated probability is 32/1000 * 100% = 3.2%.\n",
    "\n",
    "The actual probability is 4.3% and contained in the rescaled softmax probability tensor (scaled_probas[2][6])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153 x closer\n",
      "68 x every\n",
      "55 x effort\n",
      "223 x forward\n",
      "102 x inches\n",
      "50 x moves\n",
      "43 x pizza\n",
      "218 x toward\n",
      "88 x you\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123)\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "\n",
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "\n",
    "temperatures = [1, 0.1, 5]  # Original, higher, and lower temperature\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Temperature: 1\n",
      "71 x closer\n",
      "2 x every\n",
      "0 x effort\n",
      "544 x forward\n",
      "2 x inches\n",
      "1 x moves\n",
      "0 x pizza\n",
      "376 x toward\n",
      "4 x you\n",
      "\n",
      "\n",
      "Temperature: 0.1\n",
      "0 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "992 x forward\n",
      "0 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "8 x toward\n",
      "\n",
      "\n",
      "Temperature: 5\n",
      "153 x closer\n",
      "68 x every\n",
      "55 x effort\n",
      "223 x forward\n",
      "102 x inches\n",
      "50 x moves\n",
      "43 x pizza\n",
      "218 x toward\n",
      "88 x you\n",
      "\n",
      "\n",
      "Temperature: 50\n",
      "133 x closer\n",
      "98 x every\n",
      "117 x effort\n",
      "128 x forward\n",
      "114 x inches\n",
      "96 x moves\n",
      "97 x pizza\n",
      "126 x toward\n",
      "91 x you\n"
     ]
    }
   ],
   "source": [
    "for i, probas in enumerate(scaled_probas):\n",
    "    print(\"\\n\\nTemperature:\", temperatures[i])\n",
    "    print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that sampling offers an approximation of the actual probabilities when the word \"pizza\" is sampled. E.g., if it is sampled 32/1000 times, the estimated probability is 3.2%. To obtain the actual probability, we can check the probabilities directly by accessing the corresponding entry in scaled_probas.\n",
    "\n",
    "Since \"pizza\" is the 7th entry in the vocabulary, for the temperature of 5, we obtain it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0430)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp5_idx = 2\n",
    "pizza_idx = 6\n",
    "\n",
    "scaled_probas[temp5_idx][pizza_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 5.2\n",
    "Play around with different temperatures and top-k settings. Based on your observations, can you think of applications where lower temperature and top-k settings are desired? Vice versa, can you think of applications where higher temperature and top-k settings are preferred? (It's recommended to also revisit this exercise at the end of the chapter after loading the pretrained weights from OpenAI.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you know began, my a good him--I looked up his glory\"Oh\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=5,\n",
    "    temperature=5\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both temperature and top-k settings have to be adjusted based on the individual LLM (a kind of trial and error process until it generates desirable outputs)\n",
    "\n",
    "The desirable outcomes are also application-specific, though\n",
    "\n",
    "Lower top-k and temperatures result in less random outcomes, which is desired when creating educational content, technical writing or question answering, data analyses, code generation, and so forth\n",
    "\n",
    "Higher top-k and temperatures result in more diverse and random outputs, which is more desirable for brainstorming tasks, creative writing, and so forth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 5.3\n",
    "What are the different combinations of settings for the generate function to force deterministic behavior, that is, disabling the random sampling such that it always produces the same outputs similar to the generate_simple function?   So far, we covered how to pretrain LLMs and use them to generate text. The last two sections of this chapter will discuss how we save and load the trained LLM and how we load pretrained weights from OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Deterministic behavior: No top_k, no temperature scaling\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=None,\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 5.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 5.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
